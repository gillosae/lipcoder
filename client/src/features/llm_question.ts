import * as vscode from 'vscode';
import { speakTokenList, TokenChunk } from '../audio';
import { log, logError, logSuccess } from '../utils';
import { callLLMForCompletion } from '../llm';

interface LLMQuestionResult {
    question: string;
    answer: string;
    language: 'korean' | 'english';
}

/**
 * Main function to ask general questions to LLM and get speech-only responses
 */
export async function askLLMQuestion(question: string): Promise<void> {
    if (!question || !question.trim()) {
        await showAndSpeakResult({
            question: '',
            answer: "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
            language: 'korean'
        });
        return;
    }

    try {
        log(`[LLMQuestion] Processing question: "${question}"`);
        
        // Analyze the question and generate response
        const result = await processLLMQuestion(question);
        
        // Show and speak the result (speech-only, non-blocking notification)
        await showAndSpeakResult(result);
        
    } catch (error) {
        logError(`[LLMQuestion] Error processing question: ${error}`);
        
        // Determine language for error message
        const isKorean = /[ã„±-ã…|ã…-ã…£|ê°€-í£]/.test(question);
        const errorMessage = isKorean 
            ? `ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error}`
            : `Sorry, I encountered an error while processing your question: ${error}`;
            
        await showAndSpeakResult({
            question,
            answer: errorMessage,
            language: isKorean ? 'korean' : 'english'
        });
    }
}

/**
 * Process the LLM question and generate response
 */
async function processLLMQuestion(question: string): Promise<LLMQuestionResult> {
    // Detect language of the question
    const isKorean = /[ã„±-ã…|ã…-ã…£|ê°€-í£]/.test(question);
    const language: 'korean' | 'english' = isKorean ? 'korean' : 'english';
    
    // Create system prompt based on detected language
    const systemPrompt = isKorean 
        ? `ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ìµœëŒ€í•œ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. 1-2ë¬¸ì¥ìœ¼ë¡œ ìš”ì ë§Œ ë§í•´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ ì •ë³´ëŠ” ìƒëµí•˜ì„¸ìš”.`
        : `You are a helpful AI assistant. Please provide accurate and useful answers to the user's questions in English. Your responses must be extremely concise and to the point. Answer in 1-2 sentences maximum, focusing only on the essential information. Avoid unnecessary explanations or additional details.`;
    
    try {
        // Use the existing LLM completion function with appropriate parameters
        const answer = await callLLMForCompletion(
            systemPrompt,
            question,
            150, // max tokens - keep responses very concise
            0.1  // lower temperature for more focused responses
        );
        
        if (!answer || answer.trim() === '') {
            const fallbackAnswer = isKorean 
                ? "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
                : "Sorry, I couldn't generate an answer.";
            
            return {
                question,
                answer: fallbackAnswer,
                language
            };
        }
        
        return {
            question,
            answer: answer.trim(),
            language
        };
        
    } catch (error) {
        logError(`[LLMQuestion] LLM completion failed: ${error}`);
        
        // Fallback response
        const fallbackAnswer = isKorean 
            ? "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLM ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            : "Sorry, I can't generate an answer right now. Please check your LLM configuration.";
            
        return {
            question,
            answer: fallbackAnswer,
            language
        };
    }
}

/**
 * Show result in popup and speak it (speech-only, non-blocking)
 */
async function showAndSpeakResult(result: LLMQuestionResult): Promise<void> {
    const { question, answer, language } = result;
    
    // Show non-blocking notification popup (bottom-right corner) [[memory:6411078]]
    // Keep it simple and non-intrusive
    const shortAnswer = answer.length > 100 ? answer.substring(0, 97) + '...' : answer;
    vscode.window.showInformationMessage(`ğŸ’¬ ${shortAnswer}`);
    
    // Speak the full answer as plain text [[memory:6411078]]
    await speakLLMAnswer(answer);
    
    logSuccess(`[LLMQuestion] Question answered: "${question}" (${language})`);
}

/**
 * Convert LLM answer to speech using simple TTS [[memory:6411078]]
 */
async function speakLLMAnswer(answer: string): Promise<void> {
    try {
        log(`[LLMQuestion] Speaking answer: "${answer.substring(0, 100)}..."`);
        
        // Speak as simple plain text without code reading style [[memory:6411078]]
        const chunks: TokenChunk[] = [{
            tokens: [answer],
            category: undefined  // No category = plain text TTS
        }];
        
        await speakTokenList(chunks);
        
    } catch (error) {
        logError(`[LLMQuestion] Error speaking answer: ${error}`);
    }
}

/**
 * Register LLM question commands
 */
export function registerLLMQuestion(context: vscode.ExtensionContext) {
    // Register command for manual LLM questions
    context.subscriptions.push(
        vscode.commands.registerCommand('lipcoder.askLLMQuestion', async () => {
            const question = await vscode.window.showInputBox({
                placeHolder: 'Ask any question... (ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...)',
                prompt: 'LLM Question: Ask about anything - coding, math, general knowledge, etc.',
                value: '',
                ignoreFocusOut: true
            });

            if (question) {
                await askLLMQuestion(question);
            }
        })
    );
    
    // Register test command for debugging
    context.subscriptions.push(
        vscode.commands.registerCommand('lipcoder.testLLMQuestion', async () => {
            // Test with a Korean question
            await askLLMQuestion('ì‚¬ì¸ í•¨ìˆ˜ê°€ ë­ì•¼?');
        })
    );
    
    logSuccess('[LLMQuestion] LLM question commands registered');
}
